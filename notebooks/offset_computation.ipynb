{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocnet.matcher import subpixel as sp\n",
    "from autocnet.transformation import homography as hg\n",
    "from plio.io.io_gdal import GeoDataset\n",
    "from plio.geofuncs import geofuncs\n",
    "\n",
    "import affine\n",
    "import sys\n",
    "import gdal\n",
    "from osgeo import ogr\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function osgeo.gdal.WarpOptions(options=[], format='GTiff', outputBounds=None, outputBoundsSRS=None, xRes=None, yRes=None, targetAlignedPixels=False, width=0, height=0, srcSRS=None, dstSRS=None, srcAlpha=False, dstAlpha=False, warpOptions=None, errorThreshold=None, warpMemoryLimit=None, creationOptions=None, outputType=0, workingType=0, resampleAlg=None, srcNodata=None, dstNodata=None, multithread=False, tps=False, rpc=False, geoloc=False, polynomialOrder=None, transformerOptions=None, cutlineDSName=None, cutlineLayer=None, cutlineWhere=None, cutlineSQL=None, cutlineBlend=None, cropToCutline=False, copyMetadata=True, metadataConflictValue=None, setColorInterpretation=False, callback=None, callback_data=None)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gdal.Warp?\n",
    "gdal.WarpOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "\n",
    "# Environment.yml for deploy\n",
    "# Create tiffs from cubes and clip to overlap using the warp/translate\n",
    "# Plot axes/label axes\n",
    "# Function documentation\n",
    "# Bin script tiff creation\n",
    "# Analysis bin script\n",
    "    # - Quiver png\n",
    "    # - Homography.txt (9 numbers)\n",
    "    # - Dataframe to csv (keep names the same)\n",
    "    # - Stats to csv (Dataframe describe data + quantile data(x, y, magnitude))\n",
    "\n",
    "# Box to Histogram\n",
    "# Correlation/Coregistration in Z dimension\n",
    "    # - Apply homography\n",
    "    # - Difference the two DTMs for some Z offset\n",
    "\n",
    "#-------Nice To Have-------\n",
    "# Specify meters or pixels for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the paths to cubes and tiffs\n",
    "cub_image1 = \"/Volumes/Blueman/HiRISE_Jezero/ESP_023379_1985_1m_o_isis3.cub\"\n",
    "cub_image2 = \"/Volumes/Blueman/HiRISE_Jezero/ESP_048908_1985_1m_o_isis3.cub\"\n",
    "cub_image3 = \"/Volumes/Blueman/HiRISE_Jezero/DEM_1m_Jezero_C_isis3.cub\"\n",
    "\n",
    "tiff_image1 = \"/Volumes/Blueman/HiRISE_Jezero/ESP_023379_1985_1m_o_isis3.tiff\"\n",
    "tiff_image2 = \"/Volumes/Blueman/HiRISE_Jezero/ESP_048908_1985_1m_o_isis3.tiff\"\n",
    "\n",
    "# Define the resample type used by the warp command\n",
    "resample = 'bilinear'\n",
    "\n",
    "# Uncomment on first run as you will want to the tiffs\n",
    "# Set target extent\n",
    "# gdal.Warp(tiff_image1, cub_image1, resampleAlg=resample)\n",
    "# gdal.Warp(tiff_image2, cub_image2, resampleAlg=resample)\n",
    "\n",
    "# Get all geo data for the two co-registered tiffs\n",
    "geo1 = GeoDataset(tiff_image1)\n",
    "geo2 = GeoDataset(tiff_image2)\n",
    "\n",
    "# Setup and redefine all 0 values as NaNs\n",
    "arr_image1 = geo1.read_array(1)\n",
    "arr_image1[arr_image1 == 0] = np.NAN\n",
    "\n",
    "arr_image2 = geo2.read_array(1)\n",
    "arr_image2[arr_image2 == 0] = np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.loads(geo1.footprint.ExportToJson())['coordinates'][0][0]\n",
    "# geofuncs.is_clockwise([json.loads(geo1.footprint.ExportToJson())['coordinates'][0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the images and there overlapping grid area\n",
    "def show_initial_coregistration(geo_source, geo_destination, source_image, destination_image):\n",
    "    ul1, lr1 = geo1.latlon_extent\n",
    "    min_lon1, max_lon1, min_lat1, max_lat1 = lr1[0], ul1[0], ul1[1], lr1[1]\n",
    "\n",
    "    ul2, lr2 = geo2.latlon_extent\n",
    "    min_lon2, max_lon2, min_lat2, max_lat2 = lr2[0], ul2[0], ul2[1], lr2[1]\n",
    "\n",
    "    plt.figure(0, figsize=(10, 10))\n",
    "    plt.imshow(arr_image1, extent=[min_lat1, max_lat1, min_lon1, max_lon1], alpha=.5, cmap='Greys')\n",
    "    plt.imshow(arr_image2, extent=[min_lat2, max_lat2, min_lon2, max_lon2], alpha=.5, cmap='Greys')\n",
    "    plt.show()\n",
    "\n",
    "# Show the quiver plot of the offsets\n",
    "def display_quiver(comp_df, source_image, mask = [], scale = 100, scale_units = 'inches', **kwargs):\n",
    "    if len(mask) != 0:\n",
    "        comp_df = comp_df[mask]\n",
    "    \n",
    "    plt.figure(4, figsize=(20, 20))\n",
    "    plt.imshow(source_image, cmap=\"Greys\")\n",
    "    plt.quiver(comp_df['destination_x'], comp_df['destination_y'], \n",
    "               -(comp_df['xoff']), (comp_df['yoff']),\n",
    "               color = 'Red', scale = scale, scale_units = scale_units, **kwargs)\n",
    "    plt.show()\n",
    "    \n",
    "# Given an index in the dataframe examine the before and after\n",
    "# when the offset is applied\n",
    "def examine_point(idx, size, comp_df, source_image, destination_image, mask = []):\n",
    "    if len(mask) != 0:\n",
    "        comp_df = comp_df[mask]\n",
    "        \n",
    "    plt.figure(2, figsize=(5, 5))\n",
    "    plt.text(20, 50, 'Before Offset Correction', fontsize=12)\n",
    "    x, y = int(comp_df.iloc[idx]['source_x']), int(comp_df.iloc[idx]['source_y'])\n",
    "    plt.imshow(source_image[y - size:y + size, x - size:x + size], alpha = .5, cmap='Greys')\n",
    "\n",
    "    x, y = int(comp_df.iloc[idx]['destination_x']), int(comp_df.iloc[idx]['destination_y'])\n",
    "    plt.imshow(destination_image[y - size:y + size, x - size:x + size], alpha = .5, cmap='Greys')\n",
    "\n",
    "    plt.figure(3, figsize=(5, 5))\n",
    "    plt.text(20, 50, 'After Offset Correction', fontsize=12)\n",
    "    x, y = int(comp_df.iloc[idx]['offset_source_x']), int(comp_df.iloc[idx]['offset_source_y'])\n",
    "    plt.imshow(source_image[y - size: y + size, x - size: x + size], alpha = .5, cmap='Greys')\n",
    "\n",
    "    x, y = int(comp_df.iloc[idx]['destination_x']), int(comp_df.iloc[idx]['destination_y'])\n",
    "    plt.imshow(destination_image[y - size:y + size, x - size:x + size], alpha = .5, cmap='Greys')\n",
    "    plt.show()\n",
    "    \n",
    "    offset_x, offset_y, corr = comp_df.iloc[idx][['xoff', 'yoff', 'corr']]\n",
    "    print('X Offset: {}\\nY Offset: {}\\nCorrelation: {}'.format(offset_x, offset_y, corr))\n",
    "    \n",
    "def compute_homography(comp_df):\n",
    "    x1 = np.array([*zip(comp_df['offset_source_x'].__array__(), comp_df['offset_source_y'].__array__())])\n",
    "    x2 = np.array([*zip(comp_df['destination_x'].__array__(), comp_df['destination_y'].__array__())])\n",
    "    H, mask = hg.compute_homography(x1, x2)\n",
    "    \n",
    "    return H, mask\n",
    "\n",
    "# Apply the homography to the source image and display\n",
    "def apply_homography(comp_df, source_image, destination_image):\n",
    "    H,mask = compute_homography(comp_df)\n",
    "\n",
    "    w, h = source_image.shape\n",
    "    result = cv2.warpPerspective(source_image, H, (h, w))\n",
    "    result[result == 0] = np.NAN\n",
    "\n",
    "    plt.figure(0, figsize=(20, 20))\n",
    "    plt.imshow(result, cmap='Greys', alpha = .5)\n",
    "    plt.imshow(destination_image, cmap='Greys', alpha = .5)\n",
    "    \n",
    "def generate_point_grid(geo_source, geo_destination, size):\n",
    "    # Get the lat lon corners\n",
    "    lat1 = [i[0] for i in geo1.latlon_corners]\n",
    "    lon1 = [i[1] for i in geo1.latlon_corners]\n",
    "\n",
    "    # Compute a ogr geometry for the tiff which\n",
    "    # provides leverage for overlaps\n",
    "    ring1 = ogr.Geometry(ogr.wkbLinearRing)\n",
    "    for point in [*zip(lon1, lat1)]:\n",
    "        ring1.AddPoint(*point)\n",
    "    ring1.AddPoint(lon1[0], lat1[0])\n",
    "\n",
    "    poly1 = ogr.Geometry(ogr.wkbPolygon)\n",
    "    poly1.AddGeometry(ring1)\n",
    "    footprint1 = poly1\n",
    "    \n",
    "    # Do the same for the next geom\n",
    "    lat2 = [i[0] for i in geo2.latlon_corners]\n",
    "    lon2 = [i[1] for i in geo2.latlon_corners]\n",
    "\n",
    "    ring2 = ogr.Geometry(ogr.wkbLinearRing)\n",
    "    for point in [*zip(lon2, lat2)]:\n",
    "        ring2.AddPoint(*point)\n",
    "    ring2.AddPoint(lon2[0], lat2[0])\n",
    "\n",
    "    poly2 = ogr.Geometry(ogr.wkbPolygon)\n",
    "    poly2.AddGeometry(ring2)\n",
    "    footprint2 = poly2\n",
    "\n",
    "    # Set the footprint to a basic bounding\n",
    "    geo1._footprint = footprint1\n",
    "    geo2._footprint = footprint2\n",
    "\n",
    "    # Compute the overlap and get the corners now that\n",
    "    # we have the geometry\n",
    "    overlap_hull = geo1.compute_overlap(geo2)[0]\n",
    "\n",
    "    # Get the lats and lons of the assocaited corners\n",
    "    overlap_lon = [i[0] for i in overlap_hull]\n",
    "    overlap_lat = [i[1] for i in overlap_hull]\n",
    "\n",
    "    # Reset the footprints to null so they aren't used\n",
    "    # later on\n",
    "#     geo1._footprint = None\n",
    "#     geo2._footprint = None\n",
    "\n",
    "    # Define a ratio so the distrabution is even\n",
    "    overlap_ratio = (max(overlap_lon) - min(overlap_lon)) / (max(overlap_lat) - min(overlap_lat))\n",
    "\n",
    "    lon = np.linspace(min(overlap_lon) + .001, max(overlap_lon) - .001, size)\n",
    "    lat = np.linspace(min(overlap_lat) + .001, max(overlap_lat) - .001, round(size/overlap_ratio))\n",
    "    print('Generateing', len(lon), 'by', len(lat), 'point grid.')\n",
    "\n",
    "    # Get the lat, lon position for the grid\n",
    "    lonv, latv = np.meshgrid(lon, lat, sparse=True)\n",
    "\n",
    "    coords = []\n",
    "\n",
    "    # Begin looping over each point in the grid\n",
    "    for lat_val in latv:\n",
    "        for lon_val in lonv[0]:\n",
    "            # Find the point in pixel space for each image and get the value\n",
    "            x1, y1  = geo1.latlon_to_pixel(lat_val[0], lon_val)\n",
    "            x2, y2  = geo2.latlon_to_pixel(lat_val[0], lon_val)\n",
    "            point_val1 = arr_image1[y1 - 1, x1 - 1]\n",
    "            point_val2 = arr_image2[y2 - 1, x2 - 1]\n",
    "\n",
    "            # If either is zero then the point should be ignored\n",
    "            # as it lies outside of the true overlap\n",
    "            if point_val1 > 0 and point_val2 > 0:\n",
    "                coords.append([x1, y1, x2, y2, lat_val[0], lon_val])\n",
    "\n",
    "    # Build dataframe after grid contruction for data storage and \n",
    "    # ease of access\n",
    "    df = pd.DataFrame(coords, columns = ['source_x', \n",
    "                                         'source_y', \n",
    "                                         'destination_x', \n",
    "                                         'destination_y', \n",
    "                                         'lat', \n",
    "                                         'lon'])\n",
    "    return df\n",
    "\n",
    "# The Meat and Potatoes of offset calculation\n",
    "def compute_offsets(df, source_geo, destination_geo, template_size, search_size):\n",
    "    # Define a template size and a search space size\n",
    "    s_img = source_geo\n",
    "\n",
    "    d_img = destination_geo\n",
    "\n",
    "    offsets = []\n",
    "\n",
    "    # Iterate through each point in the dataframe and calculate offsets\n",
    "    print('Computing Offsets')\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        x, y = row['source_x'], row['source_y']\n",
    "        s_template = sp.clip_roi(s_img, (x, y), template_size)\n",
    "\n",
    "\n",
    "        x, y = row['destination_x'], row['destination_y']\n",
    "        d_search = sp.clip_roi(d_img, (x, y), search_size)\n",
    "\n",
    "        xoff, yoff, corr = sp.subpixel_offset(s_template, d_search)\n",
    "        xoff, yoff, corr = int(xoff),int(yoff),corr\n",
    "        # Apply the offsets to the source points and \n",
    "        # save those as well\n",
    "        offset_source_x = row['source_x'] - xoff\n",
    "        offset_source_y = row['source_y'] + yoff\n",
    "        offsets.append([offset_source_x, offset_source_y, xoff, yoff, corr])\n",
    "        sys.stdout.write('%s%s\\r' % (round((idx/len(df) * 100)), '% complete'))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    off_df = pd.DataFrame(offsets, columns = ['offset_source_x', 'offset_source_y', 'xoff', 'yoff', 'corr'])\n",
    "    comp_df = df.merge(off_df, left_index=True, right_index=True)\n",
    "\n",
    "    H, mask = compute_homography(comp_df)\n",
    "    return comp_df, H, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generateing 20 by 30 point grid.\n",
      "Computing Offsets\n",
      "[77.45290100083741, 18.521557652482873, 0.0]\n",
      "[[77.45290100083741, 18.521557652482873, 0.0], [77.45290100083741, 18.159312710282453, 0.0], [77.59817045855962, 18.159312710282453, 0.0], [77.59817045855962, 18.521557652482873, 0.0], [77.45290100083741, 18.521557652482873, 0.0]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d66d13b344b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# in the grid is seperated by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_point_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeo1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeo2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcomp_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_offsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeo1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeo2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-9cff706b0e4e>\u001b[0m in \u001b[0;36mcompute_offsets\u001b[0;34m(df, source_geo, destination_geo, template_size, search_size)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0ms_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_roi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/autocnet/autocnet/matcher/subpixel.py\u001b[0m in \u001b[0;36mclip_roi\u001b[0;34m(img, center, img_size)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         clipped_img = img.read_array(pixels=[x_start, y_start,\n\u001b[0;32m---> 55\u001b[0;31m                                              x_stop + 1, y_stop + 1])\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclipped_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/plio/plio/io/io_gdal.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(self, band, pixels, dtype)\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0mxmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxy_extent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;31m# If the image is south up, flip the roi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorth_up\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m                 \u001b[0mystart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mymax\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mystart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mycount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mxstart\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/plio/plio/io/io_gdal.py\u001b[0m in \u001b[0;36mnorth_up\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfootprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExportToJson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coordinates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfootprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExportToJson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coordinates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgeofuncs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_clockwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfootprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExportToJson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coordinates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/plio/plio/geofuncs/geofuncs.py\u001b[0m in \u001b[0;36mis_clockwise\u001b[0;34m(vertices)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0marea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0may\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvertices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvertices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0marea\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mby\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0may\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Generate a dataframe of points associated with a grid where each point\n",
    "# in the grid is seperated by \n",
    "df = generate_point_grid(geo1, geo2, 20)\n",
    "comp_df, H, mask = compute_offsets(df, geo1, geo2, 25, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add units and a 99 percentile\n",
    "comp_df[['xoff', 'yoff', 'corr']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add descriptions/units\n",
    "plt.figure(0, figsize=(10, 10))\n",
    "comp_df[mask].boxplot(['xoff', 'yoff'], figsize=(10, 10))\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "comp_df[mask].boxplot('corr', figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_initial_coregistration(geo1, geo2, arr_image1, arr_image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_quiver(comp_df, arr_image2, mask = mask, scale = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "examine_point(1, 200, comp_df, arr_image1, arr_image2, mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_homography(comp_df, arr_image1, arr_image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
