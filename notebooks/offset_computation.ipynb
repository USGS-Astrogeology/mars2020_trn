{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "\n",
    "# Environment.yml for deploy\n",
    "# Create tiffs from cubes and clip to overlap using the warp/translate\n",
    "# Plot axes/label axes\n",
    "# Function documentation\n",
    "# Bin script tiff creation\n",
    "# Analysis bin script\n",
    "    # - Quiver png\n",
    "    # - Homography.txt (9 numbers)\n",
    "    # - Dataframe to csv (keep names the same)\n",
    "    # - Stats to csv (Dataframe describe data + quantile data(x, y, magnitude))\n",
    "\n",
    "# Box to Histogram\n",
    "# Correlation/Coregistration in Z dimension\n",
    "    # - Apply homography\n",
    "    # - Difference the two DTMs for some Z offset\n",
    "\n",
    "#-------Nice To Have-------\n",
    "# Specify meters or pixels for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Be sure to switch the kernel to autocnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Notebook\n",
    "\n",
    "Run these first two cells to define imports and the functions used in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import gdal\n",
    "import math\n",
    "import affine\n",
    "from osgeo import ogr\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from plio.geofuncs import geofuncs\n",
    "from plio.io.io_gdal import GeoDataset\n",
    "from autocnet.matcher import subpixel as sp\n",
    "from autocnet.transformation import homography as hg\n",
    "from autocnet.camera import camera\n",
    "from autocnet.transformation.fundamental_matrix import compute_fundamental_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'plasma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a pandas dataframe of the image information you are currently using\n",
    "def image_summary(cub_image1, cub_image2, tiff_geo1, tiff_geo2):\n",
    "    \n",
    "    # Allows dataframe to show entire result\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    columns = {'File Type': [cub_image1.split('.')[-1], cub_image2.split('.')[-1]],\n",
    "               'File Path': [cub_image1, cub_image2],\n",
    "               'Pixel Resolution': [tiff_geo1.pixel_width, tiff_geo2.pixel_width], \n",
    "               'Units': [tiff_geo1.scale[0], tiff_geo2.scale[0]],\n",
    "               'Image Name': [tiff_geo1, tiff_geo2]}\n",
    "    \n",
    "    df = pd.DataFrame(data = columns)\n",
    "    \n",
    "    # Reorder the dataframe columnds to th e right order\n",
    "    df = df[['Image Name', 'Pixel Resolution', 'Units','File Type', 'File Path']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Plot the images and there overlapping grid area\n",
    "def show_coregistration(source_image, destination_image, **kwargs):\n",
    "    plt.figure(0, figsize=(10, 10))\n",
    "    plt.imshow(source_image, **kwargs)\n",
    "    plt.imshow(destination_image, **kwargs)\n",
    "    plt.show()\n",
    "\n",
    "# Show the quiver plot of the offsets\n",
    "def display_quiver(comp_df, source_image, mask = [], scale = 100, scale_units = 'inches', **kwargs):\n",
    "    if len(mask) != 0:\n",
    "        comp_df = comp_df[mask]\n",
    "    plt.imshow(source_image, cmap=\"Greys\", alpha = .5)\n",
    "    plt.quiver(comp_df['destination_x'], comp_df['destination_y'], \n",
    "               -(comp_df['xoff']), (comp_df['yoff']),\n",
    "               color = 'Red', scale = scale, scale_units = scale_units, **kwargs)\n",
    "    \n",
    "# Given an index in the dataframe examine the before and after\n",
    "# when the offset is applied\n",
    "def examine_point(idx, size, comp_df, source_image, destination_image, mask = [], **kwargs):\n",
    "    if len(mask) != 0:\n",
    "        comp_df = comp_df[mask]\n",
    "        \n",
    "    plt.figure(2, figsize=(5, 5))\n",
    "    plt.text(20, 50, 'Before Offset Correction', fontsize=12)\n",
    "    x, y = int(comp_df.iloc[idx]['source_x']), int(comp_df.iloc[idx]['source_y'])\n",
    "    plt.imshow(source_image[y - size:y + size, x - size:x + size], **kwargs)\n",
    "\n",
    "    x, y = int(comp_df.iloc[idx]['destination_x']), int(comp_df.iloc[idx]['destination_y'])\n",
    "    plt.imshow(destination_image[y - size:y + size, x - size:x + size], **kwargs)\n",
    "\n",
    "    plt.figure(3, figsize=(5, 5))\n",
    "    plt.text(20, 50, 'After Offset Correction', fontsize=12)\n",
    "    x, y = int(comp_df.iloc[idx]['offset_source_x']), int(comp_df.iloc[idx]['offset_source_y'])\n",
    "    plt.imshow(source_image[y - size: y + size, x - size: x + size], **kwargs)\n",
    "\n",
    "    x, y = int(comp_df.iloc[idx]['destination_x']), int(comp_df.iloc[idx]['destination_y'])\n",
    "    plt.imshow(destination_image[y - size:y + size, x - size:x + size], **kwargs)\n",
    "    plt.show()\n",
    "    \n",
    "    offset_x, offset_y, corr = comp_df.iloc[idx][['xoff', 'yoff', 'corr']]\n",
    "    print('X Offset: {}\\nY Offset: {}\\nCorrelation: {}'.format(offset_x, offset_y, corr))\n",
    "    \n",
    "def compute_homography(comp_df):\n",
    "    x1 = np.array([*zip(comp_df['offset_source_x'].__array__(), comp_df['offset_source_y'].__array__())])\n",
    "    x2 = np.array([*zip(comp_df['destination_x'].__array__(), comp_df['destination_y'].__array__())])\n",
    "    H, mask = hg.compute_homography(x1, x2)\n",
    "    \n",
    "    return H, mask\n",
    "\n",
    "# Apply the homography to the source image and display\n",
    "def apply_homography(comp_df, image, H, height, width):\n",
    "    result = cv2.warpPerspective(image, H, (height, width))\n",
    "    \n",
    "    img_min = np.nanmin(image)\n",
    "    img_max = np.nanmax(image)\n",
    "    \n",
    "    result[result > img_max] = np.NAN\n",
    "    result[result < img_min] = np.NAN\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def generate_point_grid(source_geo, destination_geo, source_raster, destination_raster, size):\n",
    "    # Compute the overlap and get the corners now that\n",
    "    # we have the geometry\n",
    "    overlap_hull = source_geo.compute_overlap(destination_geo)[0]\n",
    "\n",
    "    # Get the lats and lons of the assocaited corners\n",
    "    overlap_lon = [i[0] for i in overlap_hull]\n",
    "    overlap_lat = [i[1] for i in overlap_hull]\n",
    "\n",
    "    # Define a ratio so the distrabution is even\n",
    "    overlap_ratio = (max(overlap_lon) - min(overlap_lon)) / (max(overlap_lat) - min(overlap_lat))\n",
    "\n",
    "    lon = np.linspace(min(overlap_lon) + .001, max(overlap_lon) - .001, size)\n",
    "    lat = np.linspace(min(overlap_lat) + .001, max(overlap_lat) - .001, round(size/overlap_ratio))\n",
    "\n",
    "    # Get the lat, lon position for the grid\n",
    "    lonv, latv = np.meshgrid(lon, lat, sparse=True)\n",
    "    print('Generating a', len(lonv[0]), 'by', len(latv), 'point grid')\n",
    "\n",
    "    coords = []\n",
    "\n",
    "    # Begin looping over each point in the grid\n",
    "    for lat_val in latv:\n",
    "        for lon_val in lonv[0]:\n",
    "            # Find the point in pixel space for each image and get the value\n",
    "            x1, y1  = source_geo.latlon_to_pixel(lat_val[0], lon_val)\n",
    "            x2, y2  = destination_geo.latlon_to_pixel(lat_val[0], lon_val)\n",
    "            point_val1 = source_raster[y1 - 1, x1 - 1]\n",
    "            point_val2 = destination_raster[y2 - 1, x2 - 1]\n",
    "\n",
    "            # If either is zero then the point should be ignored\n",
    "            # as it lies outside of the true overlap\n",
    "            if point_val1 > 0 and point_val2 > 0:\n",
    "                coords.append([x1, y1, x2, y2, lat_val[0], lon_val])\n",
    "\n",
    "    # Build dataframe after grid contruction for data storage and \n",
    "    # ease of access\n",
    "    df = pd.DataFrame(coords, columns = ['source_x', \n",
    "                                         'source_y', \n",
    "                                         'destination_x', \n",
    "                                         'destination_y', \n",
    "                                         'lat', \n",
    "                                         'lon'])\n",
    "    return df\n",
    "\n",
    "# The Meat and Potatoes of offset calculation\n",
    "def compute_offsets(df, s_img, d_img, template_size, search_size, corr_threshold=0.9):\n",
    "    offsets = []\n",
    "\n",
    "    # Iterate through each point in the dataframe and calculate offsets\n",
    "    print('Computing Offsets')\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        x, y = row['source_x'], row['source_y']\n",
    "        s_template = sp.clip_roi(s_img, (x, y), template_size)\n",
    "\n",
    "\n",
    "        x, y = row['destination_x'], row['destination_y']\n",
    "        d_search = sp.clip_roi(d_img, (x, y), search_size)\n",
    "\n",
    "        xoff, yoff, corr = sp.subpixel_offset(s_template, d_search)\n",
    "        mag = np.linalg.norm([xoff, yoff])\n",
    "        # Apply the offsets to the source points and \n",
    "        # save those as well\n",
    "        offset_source_x = row['source_x'] - xoff\n",
    "        offset_source_y = row['source_y'] + yoff\n",
    "        # Rotation based on a flipped y axis and a 90 degree rotate by axis from 0 to 360\n",
    "        rotation = (((math.atan2(-yoff, -xoff) / math.pi * 180) + 90) +360) % 360\n",
    "        offsets.append([offset_source_x, offset_source_y, xoff, yoff, mag, corr, rotation])\n",
    "        sys.stdout.write('%s%s\\r' % (round((idx/len(df) * 100)), '% complete'))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    off_df = pd.DataFrame(offsets, columns = ['offset_source_x', 'offset_source_y', 'xoff', 'yoff', 'mag', 'corr', 'rotation'])\n",
    "    comp_df = df.merge(off_df, left_index=True, right_index=True)\n",
    "    corr_mask = comp_df['corr'] > corr_threshold\n",
    "    \n",
    "    H, mask = compute_homography(comp_df[corr_mask])\n",
    "    mask_index = comp_df[corr_mask][mask].index.__array__()\n",
    "    full_mask = [True if i in mask_index else False for i, val in comp_df.iterrows()]\n",
    "    return comp_df, H, full_mask\n",
    "\n",
    "# Function to create CDF graphs for offsets\n",
    "def calculate_cdf_graphs(comp_df, step = .01, df_mask = True):\n",
    "\n",
    "    if df_mask:\n",
    "        comp_df = comp_df[mask]\n",
    "    \n",
    "    def generate_cdf_graph(column, step, **kwargs):\n",
    "        # Set min/maxes for the column\n",
    "        max_bin, min_bin = np.max(comp_df[column]), np.min(comp_df[column])\n",
    "        \n",
    "        # Get cumulative data and the count\n",
    "        bins, count, cumulative = cumulative_stats(comp_df, column_name = column, bin_start = min_bin, bin_end = max_bin, bin_step = step, df_mask = False)\n",
    "        \n",
    "        # Use the min and max to determine the base for each column\n",
    "        base = np.linspace(min_bin, max_bin + 1, len(cumulative))\n",
    "        \n",
    "        # Plot the cumulative function\n",
    "        plt.plot(base, cumulative * 100, **kwargs)\n",
    "    \n",
    "    \n",
    "    plt.figure(0, figsize=(15,15))\n",
    "    generate_cdf_graph(\"mag\", step, label= \"Magnitude\", c='b', linestyle='--')\n",
    "    generate_cdf_graph(\"xoff\", step, label= \"Xoff\", c='r', linestyle=':')\n",
    "    generate_cdf_graph(\"yoff\", step, label= \"Yoff\", c='g', linestyle='-.')\n",
    "    plt.legend(loc=5, fontsize=\"x-large\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Setup Labels for graph\n",
    "    plt.xlabel('Pixel Offset Value', fontsize=15)\n",
    "    plt.ylabel('Percentage of Values', fontsize=15)\n",
    "    plt.title('CDF Mars2020 Pixel Offset', fontsize=18)\n",
    "    \n",
    "    if not use_default_graph_values:\n",
    "        plt.xticks(np.arange(x_start, x_end, x_step))\n",
    "        plt.yticks(np.arange(y_start, y_end, y_step))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def cumulative_stats(comp_df, column_name='mag', bin_start=0, bin_end=100, bin_step=1, df_mask=True):\n",
    "    \n",
    "    # Grabs the masked data\n",
    "    # Allows for using non-mask data\n",
    "    if df_mask:\n",
    "        data = comp_df[mask][column_name]\n",
    "    else:\n",
    "        data = comp_df[column_name]\n",
    "    \n",
    "    # Generates the bins for cumulative stats\n",
    "    bins = np.arange(bin_start, bin_end, bin_step)\n",
    "    \n",
    "    # Puts the data into bins\n",
    "    organized_data = np.digitize(data, bins)\n",
    "    \n",
    "    # Counts the number in each bin\n",
    "    count = np.bincount(organized_data)\n",
    "    \n",
    "    # Calculates total_count for percentage \n",
    "    total_count = np.sum(count)\n",
    "    count_percentage = count / total_count\n",
    "    \n",
    "    # Calculates the cumulative sum using the count_percentage array\n",
    "    cumulative_sum = np.cumsum(count_percentage)\n",
    "    \n",
    "    return bins, count, cumulative_sum\n",
    "\n",
    "def calculate_cumulative_statistics_df(comp_df, bin_step = .25):\n",
    "\n",
    "    # Setup values for min and max\n",
    "    def generate_column_df(column):\n",
    "        # Cumulative Statistics\n",
    "        max_val, min_val = np.amax(comp_df[mask][column]), np.amin(comp_df[mask][column])\n",
    "        \n",
    "        bins, count, cumulative_sum = cumulative_stats(comp_df, column_name=column, bin_start=min_val, bin_end=max_val, bin_step=bin_step)\n",
    "        bins = np.append(bins, max_val)\n",
    "        \n",
    "        # Generates the data for the DataFrame\n",
    "        columns = {column.capitalize() + ' Count': count, \n",
    "                   column.capitalize() + ' Cumulative': cumulative_sum, \n",
    "                   column.capitalize() + ' Bins': bins}\n",
    "        \n",
    "        return pd.DataFrame(data=columns)\n",
    "\n",
    "    # Generates the dataframe\n",
    "    mag_df = generate_column_df(\"mag\")\n",
    "    yoff_df = generate_column_df(\"xoff\")\n",
    "    xoff_df = generate_column_df(\"yoff\")\n",
    "\n",
    "    cum_df = pd.concat([xoff_df, yoff_df, mag_df], axis=1)\n",
    "    \n",
    "    return cum_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths and Files\n",
    "\n",
    "Similarly to the Image Conversion notebook this notebook requires a similar image setup. For now, I would copy the contents of that cell of the image conversion notebook into the cell bellow.\n",
    "Each cube requires a \"basepath\" and the cubes name.\n",
    "\n",
    "The basepath is the path to the directory containing the cubes. Ideally, you would want to place mutiple cubes within the same directory\n",
    "to make opening and accessing them easier. In the cell bellow, define various basepaths and cub names.\n",
    "\n",
    "You then can define the two cubes however you want using the basepath and a cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the paths to cubes and tiffs\n",
    "hirise_basepath = '/work/projects/mars2020_trn/test_images_jupyter/HiRISE_Jezero/'\n",
    "ctx_20_basepath = '/work/projects/mars2020_trn/test_images_jupyter/CTX_Jezero/'\n",
    "ctx_6_basepath = '/work/projects/mars2020_trn/test_images_jupyter/CTX_Jezero/'\n",
    "hrsc_basepath = '/work/projects/mars2020_trn/test_images_jupyter/HRSC_Jezero/'\n",
    "\n",
    "hirise_dem1 = os.path.join(hirise_basepath, 'DEM_1m_Jezero_CE_isis3.cub')\n",
    "hirise_dem2 = os.path.join(hirise_basepath, 'DEM_1m_Jezero_C_isis3.cub')\n",
    "\n",
    "ctx_dem1 = os.path.join(ctx_6_basepath, 'tfm_abso_Jezero_F05_V6_IAUsph_adj_XYZposAndVelAndAngles_20m_onePassAfterngate.tiff')\n",
    "ctx_dem2 = os.path.join(ctx_6_basepath, 'tfm_abso_Jezero_J03_V6_IAUsph_adj_XYZposAndVelAndAngles_20m_onePassAfterngate.tiff')\n",
    "\n",
    "# setup the paths to cubes and tiffs\n",
    "hirise_cub1 = \"ESP_023524_1985_1m_o_isis3.cub\"\n",
    "hirise_cub2 = \"ESP_048908_1985_1m_o_isis3.cub\"\n",
    "ctx_cub1 = \"F05_037607_2008_XN_20N282W_v6_PosAndVelAndAngles_20m_o.cub\"\n",
    "ctx_cub2 = \"J03_045994_1986_XN_18N282W_v6_20m_o.cub\"\n",
    "ctx_cub3 = \"F05_037607_2008_XN_20N282W_v6_PosAndVelAndAngles_6m_o.cub\"\n",
    "ctx_cub4 = \"J03_045994_1986_XN_18N282W_v6_6m_o.cub\"\n",
    "hrsc_cub = \"H5270_0000_ND4.IMG\"\n",
    "\n",
    "cub_image1 = os.path.join(hirise_basepath, hirise_cub1)\n",
    "cub_image2 = os.path.join(hirise_basepath, hirise_cub2)\n",
    "\n",
    "# cub_image1 = os.path.join(ctx_6_basepath, ctx_cub3)\n",
    "# cub_image2 = os.path.join(ctx_6_basepath, ctx_cub4)\n",
    "\n",
    "# cub_image1 = os.path.join(ctx_20_basepath, ctx_cub1)\n",
    "# cub_image2 = os.path.join(ctx_20_basepath, ctx_cub2)\n",
    "\n",
    "# cub_image1 = hirise_dem1\n",
    "# cub_image2 = hirise_dem2\n",
    "\n",
    "# cub_image1 = ctx_dem1\n",
    "# cub_image2 = ctx_dem2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables\n",
    "You can also define\n",
    "\n",
    "* Search Size \n",
    "    * Some integer i to generate and i x i search space\n",
    "    * Increasing this value will make the overall process take longer but be more precise\n",
    "* Template \n",
    "    * Size Some integer i to generate and i x i template\n",
    "    * Increasing this value will make the overall process shorter but less precise\n",
    "* Grid Size \n",
    "    * Some integer i to generate an i by j grid of points, where j is determined dynamically\n",
    "    * Increasing this value will make the overall process longer as there will be more points on the image\n",
    "* Correlation Threshold \n",
    "    * Some float <= 1 for how corrlated two points need to be to include in homography calculation\n",
    "    * Increasing this value will limit the system to points that are within the given percentile\n",
    "* Use Default Graph Values \n",
    "    * As long as this is True, it will autoscale the CDF graphs, if it is turned to False it will then use the   values defined after it to set the grid range \n",
    "* X Axis Values \n",
    "    * x_start: The x axis starting point for the range of values desired for preview on the CDF graph\n",
    "    * x_end: The x axis end point for the range of values desired for preview on the CDF graph\n",
    "    * x_step: The value of each step that will be taken between (x_start, x_end) Ex. A step value of 5 where the range of values is (0, 100) this would produce 20 ticks on the x axis.\n",
    "* Y Axis Values\n",
    "    * y_start: The y axis starting point for the range of values desired for preview on the CDF graph\n",
    "    * y_end: The y axis end point for the range of values desired for preview on the CDF graph\n",
    "    * y_step: The value of each step that will be taken between (y_start, y_end) Ex. A step value of 5 where the range of values is (0, 100) this would produce 20 ticks on the y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_size = 101\n",
    "template_size = 25\n",
    "grid_size = 20\n",
    "corr_threshold = .95\n",
    "\n",
    "use_default_graph_values = True\n",
    "\n",
    "# X Axis Values\n",
    "x_start = 0\n",
    "x_end = 100\n",
    "x_step = 1\n",
    "\n",
    "# Y Axis Values\n",
    "y_start = 0\n",
    "y_end = 100\n",
    "y_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_image1 = os.path.splitext(cub_image1)[0] + '.tiff'\n",
    "tiff_image2 = os.path.splitext(cub_image2)[0] + '.tiff'\n",
    "\n",
    "tiff_geo1 = GeoDataset(tiff_image1)\n",
    "tiff_geo2 = GeoDataset(tiff_image2)\n",
    "\n",
    "# Setup and redefine all 0 values as NaNs\n",
    "arr_image1 = tiff_geo1.read_array(1)\n",
    "arr_image1[arr_image1 == 0] = np.NaN\n",
    "\n",
    "arr_image2 = tiff_geo2.read_array(1)\n",
    "arr_image2[arr_image2 == 0] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Summary Box\n",
    "\n",
    "This information is based on the current images you are using. All information is pulled from the images themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_summary(cub_image1, cub_image2, tiff_geo1, tiff_geo2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_coregistration(arr_image1, arr_image2, alpha = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate a dataframe of points associated with a grid where each point\n",
    "# in the grid is seperated by\n",
    "df = generate_point_grid(tiff_geo1, tiff_geo2, arr_image1, arr_image2, grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comp_df, H, mask = compute_offsets(df, tiff_geo1, tiff_geo2, template_size, search_size, corr_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDF Graphs\n",
    "\n",
    "This next cell calcullates CDF graphs for the magnitude of x-offets and y-offsets, it also displays the CDF line for x-offset and y-offset individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "calculate_cdf_graphs(comp_df, step = .2, df_mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Data\n",
    "\n",
    "The three cells bellow are all focused on the statistics of the computation.\n",
    "\n",
    "## Units\n",
    "\n",
    "__Xoff__: All pixel values, displaying stats on the shift in the x direction <br>\n",
    "__Yoff__: All pixel values, displaying stats on the shift in the y direction <br>\n",
    "__Corr__: All as percentages, displaying stats on the correlation between point comparisons in the current calculation\n",
    "\n",
    "This goes for both the table bellow and the three box plots bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_file_name = 'stats.csv'\n",
    "\n",
    "# Add units / calculates stats\n",
    "stats = comp_df[mask][['xoff', 'yoff', 'mag', 'corr', 'rotation']].describe([.25, .5, .75, .99])\n",
    "\n",
    "# # Calculates rms and adds it to the dataframe\n",
    "mean_squared = np.square(stats.loc['mean'])\n",
    "std_squared = np.square(stats.loc['std'])\n",
    "rms = np.sqrt(mean_squared + std_squared)\n",
    "stats.loc['rms'] = rms\n",
    "stats = stats.sort_index()\n",
    "\n",
    "# Converts to CSV using the stats_file_name variable\n",
    "stats.to_csv(stats_file_name, index=False)\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative Statistics CSV\n",
    "\n",
    "Running the next cell will produce a CSV of the cumulative statistics of whatever column __('xoff, 'yoff', 'mag')__ you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_df = calculate_cumulative_statistics_df(comp_df, bin_step = .25)\n",
    "\n",
    "cum_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run this cell for a CSV of the statistics from above\n",
    "\n",
    "__stats_file_name__: This can be used for a regular file_name and will convert to a CSV in the same directory or it can be              used to specify a filepath to write the CSV to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name for CSV \n",
    "stats_file_name = 'stats.csv'\n",
    "\n",
    "# Creates the combined stats dataframe\n",
    "new_cum_df = pd.concat([stats, cum_df], axis=1)\n",
    "\n",
    "# Converts the dataframe to CSV\n",
    "new_cum_df.to_csv(stats_file_name, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add descriptions/units\n",
    "plot = comp_df[mask][['xoff', 'yoff', 'mag']].plot(kind='box', figsize=(10, 10))\n",
    "plot.set_ylabel('Pixel Offset (pixels)', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = comp_df['corr'][mask].plot(kind='box', figsize=(10, 10))\n",
    "plot.set_ylabel('Percentage', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiver Plot Display\n",
    "\n",
    "Uses the comp_df's x, and y offsets to generate the quiver arrows. While they seem exaggerated they size of a quiver is relative to it's magnitude. Where scale uses the magnitude to draw itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(4, figsize=(20, 20))\n",
    "display_quiver(comp_df[mask], arr_image2, scale = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiver Information to CSV\n",
    "Run the next cell to convert the comp_df to CSV, allowing you to load it into another GIS program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name for CSV \n",
    "quiver_file_name = 'name_of_quiver.csv'\n",
    "\n",
    "# Converts above cell to CSV\n",
    "comp_df.to_csv(quiver_file_name, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Point Display\n",
    "\n",
    "The first value is the point on the image from left to right, bottom to top.<br>That is, the leftmost, bottom point is 0, the next to the right is 1, etc.\n",
    "\n",
    "The second value is the size of the area to display in pixels as a square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "examine_point(17, 100, comp_df[mask], arr_image1, arr_image2, alpha = .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homography Application\n",
    "\n",
    "Uses the homography generated by the initial compute offsets function to realign the initial images.\n",
    "<br>This is only as accurate as the homography that was generated and is more or less a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_geo1 = GeoDataset(hirise_dem1)\n",
    "dem_geo2 = GeoDataset(hirise_dem2)\n",
    "\n",
    "# Setup and redefine all 0 values as NaNs\n",
    "arr_image1 = dem_geo1.read_array(1)\n",
    "arr_image1[np.isclose(arr_image1, -3.4028227e+38)] = np.NaN\n",
    "\n",
    "arr_image2 = dem_geo2.read_array(1)\n",
    "arr_image2[np.isclose(arr_image2, -3.4028227e+38)] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_coregistration(arr_image1, arr_image2, alpha = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = apply_homography(comp_df[mask], arr_image2, H, height=len(arr_image1[0]), width=len(arr_image1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_coregistration(new_image, arr_image1, alpha = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "diff = abs(new_image) - abs(arr_image1)\n",
    "plt.imshow(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = pd.Series(data = diff.flatten())\n",
    "series.dropna()\n",
    "series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
